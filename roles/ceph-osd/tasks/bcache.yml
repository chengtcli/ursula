---
# Create hybrid bucket
- name: Create root hybrid-bucket
  shell: ceph osd crush add-bucket {{ hybrid_pool }} root
  register: result
  changed_when: result.stdout |search("added bucket")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create crush ruleset for the hybrid root bucket
- name: Create crush ruleset hybrid root bucket
  shell: ceph osd crush rule create-simple {{ hybrid_ruleset }} {{ hybrid_pool }} host firstn
  register: result
  changed_when: not result.stdout |search("already exists")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create osd host bucket
- name: Create osd host bucket
  shell: ceph osd crush add-bucket {{ ansible_hostname }} host
  register: result
  changed_when: result.stdout |search("added bucket")

# Move the host bucket under corresponding root
- name: Move host bucket under root bucket
  shell: ceph osd crush move {{ ansible_hostname }} root={{ hybrid_pool }}
  register: result
  changed_when: result.stdout |search("moved item")

# we check for journal and bcache partitions to skip destructive tasks below
- name: check if journal partitions exist on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'journal'"
  failed_when: false
  changed_when: false
  register: journal_partitions

- name: check if bcache partition exists on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'bcache'"
  failed_when: false
  changed_when: false
  register: bcache_partition

- name: initialize ceph_init_ssd flag
  set_fact:
    ceph_init_ssd: false

- name: set ceph_init_ssd flag
  set_fact:
    ceph_init_ssd: true
  when:
    - journal_partitions.rc != 0
    - bcache_partition.rc != 0

# block initial ssd disk
- block:
  - name: install pexpect python module
    pip: name=pexpect

  - name: copy installer of intel cas
    copy: src=installer-Intel-CAS-03.01.01.10140400-trial.run dest=/tmp/installer-Intel-CAS-03.01.01.10140400-trial.run mode=0555

  - name: install intel cas
    expect:
      command: /bin/bash -c "which casadm || /tmp/installer-Intel-CAS-03.01.01.10140400-trial.run"
      responses:
        .*--More--\S*: ''
        .*I have read and accept the license.*: 'y'
        .*Do you want to continue.*: 'y'
  - name: mklabel gpt
    command: "parted -s /dev/{{ ceph.bcache_ssd_device }} mklabel gpt"

  - name: make journal partitions
    command: parted --script /dev/{{ ceph.bcache_ssd_device }}
             mkpart journal {{ (item|int * 10000) + 1 }}MiB {{ (item|int * 10000) + 10000 }}MiB
    with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"

  - name: make bcache partition
    command: parted --script /dev/{{ ceph.bcache_ssd_device }}
             mkpart bcache {{ ceph.disks|length * 10000 + 1 }}MiB 100%

  - name: make-bcache -C <ssd device>
    command: casadm --start-cache --cache-device /dev/{{ ceph.bcache_ssd_device }}{{ ceph.disks|length + 1 }} --cache-mode wb
  when: ceph_init_ssd

- name: make xfs on bcache devices
  command: mkfs -t xfs -f -i size=2048 -- /dev/{{ item }}
           creates=/sys/block/{{ item }}/inteldisk*
  with_items: "{{ ceph.disks }}"

- name: make-bcache -B <sata disks>
  command: casadm --add-core --cache-id 1 --core-device /dev/{{ item }}
           creates=/sys/block/{{ item }}/inteldisk*
  register: result_bcache_backs
  with_items: "{{ ceph.disks }}"

- name: wait for bcache directories to be created before running next task
  wait_for: path=/sys/block/intelcas1-{{ item }} timeout=30
  with_sequence: "start=1 end={{ ceph.disks|length }}"

- name: make sure all uuids of bcaches exist
  shell: test `ls -l /dev/disk/by-uuid/ |grep intelcas |wc -l` -eq "{{ ceph.disks|length }}"
  register: result
  until: result.rc == 0
  retries: 6
  delay: 10

- name: activate osds
  ceph_bcache:
    disks: "{{ ceph.disks }}"
    ssd_device: "{{ ceph.bcache_ssd_device }}"
    ceph_init_ssd: "{{ ceph_init_ssd }}"
    journal_guid: "{{ ceph.journal_guid }}"
  when: ceph_init_ssd or result_bcache_backs.changed

- name: make sure ceph-osd-all started
  service: name=ceph-osd-all state=started

# pool pg number is based on osd amount
# we should create pool when all osds are up
- name: create openstack pool
  ceph_pool:
    pool_name: "{{ hybrid_pool }}"
    osds: "{{ groups['ceph_osds_hybrid']|length * ceph.disks|length }}"
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds_hybrid'][-1]

# not sure is there a better way to get ruleset id
- name: set ruleset for pool
  shell: ceph osd pool set {{ hybrid_pool }} crush_ruleset
         $(ceph osd crush rule dump |grep {{ hybrid_ruleset }} -A1 |grep -oP '\d+')
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds_hybrid'][-1]
