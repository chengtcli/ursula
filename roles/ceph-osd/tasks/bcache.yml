---
# Create hybrid bucket
- name: Create root hybrid-bucket
  shell: ceph osd crush add-bucket {{ hybrid_pool }} root
  register: result
  changed_when: result.stdout |search("added bucket")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create crush ruleset for the hybrid root bucket
- name: Create crush ruleset hybrid root bucket
  shell: ceph osd crush rule create-simple {{ hybrid_ruleset }} {{ hybrid_pool }} host firstn
  register: result
  changed_when: not result.stdout |search("already exists")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create osd host bucket
- name: Create osd host bucket
  shell: ceph osd crush add-bucket {{ ansible_hostname }} host
  register: result
  changed_when: result.stdout |search("added bucket")

# Move the host bucket under corresponding root
- name: Move host bucket under root bucket
  shell: ceph osd crush move {{ ansible_hostname }} root={{ hybrid_pool }}
  register: result
  changed_when: result.stdout |search("moved item")

# we check for journal and bcache partitions to skip destructive tasks below
- name: check if journal partitions exist on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'journal'"
  ignore_errors: true
  changed_when: false
  register: journal_partitions

- name: check if bcache partition exists on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'bcache'"
  ignore_errors: true
  changed_when: false
  register: bcache_partition

- name: initialize ok_to_deploy flag
  set_fact:
    ok_to_deploy: false

- name: set ok_to_deploy flag
  set_fact:
    ok_to_deploy: true
  when: journal_partitions.rc != 0 and
        bcache_partition.rc != 0

- name: mklabel gpt
  command: "parted -s /dev/{{ ceph.bcache_ssd_device }} mklabel gpt"
  when: ok_to_deploy

- name: make journal partitions
  command: parted --script /dev/{{ ceph.bcache_ssd_device }}
           mkpart journal {{ (item|int * 10000) + 1 }}MiB {{ (item|int * 10000) + 10000 }}MiB
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: make bcache partition
  command: parted --script /dev/{{ ceph.bcache_ssd_device }}
           mkpart bcache {{ ceph.disks|length * 10000 + 1 }}MiB 100%
  when: ok_to_deploy

- name: make-bcache -C <ssd device>
  command: make-bcache -C /dev/{{ ceph.bcache_ssd_device }}{{ ceph.disks|length + 1 }}
  ignore_errors: true
  when: ok_to_deploy

- name: make-bcache -B <sata disks>
  command: "make-bcache -B /dev/{{ item }}"
  with_items: "{{ ceph.disks }}"
  when: ok_to_deploy

- name: udevadm trigger
  command: "udevadm trigger"
  when: ok_to_deploy

- name: wait for bcache directories to be created before running next task
  wait_for: path=/sys/block/bcache{{ item }}
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: set cache mode to writeback
  shell: echo writeback > /sys/block/bcache{{ item }}/bcache/cache_mode
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: determine bcache uuid
  shell: bcache-super-show /dev/{{ ceph.bcache_ssd_device }}{{ ceph.disks|length + 1 }} |
         grep cset.uuid | awk '{print $2}'
  changed_when: false
  register: bcache_uuid
  when: ok_to_deploy

- name: attach to bcache devices
  shell: echo {{ bcache_uuid.stdout }} > /sys/block/bcache{{ item }}/bcache/attach
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: make xfs on bcache devices
  command: "mkfs -t xfs -f -i size=2048 -- /dev/bcache{{ item }}"
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: activate osds
  ceph_bcache:
    disks: "{{ ceph.disks }}"
    ssd_device: "{{ ceph.bcache_ssd_device }}"
    journal_guid: "{{ ceph.journal_guid }}"
  when: ok_to_deploy

# pool pg number is based on osd amount
# we should create pool when all osds are up
- name: create openstack pool
  ceph_pool:
    pool_name: "{{ hybrid_pool }}"
    osds: "{{ groups['ceph_osds_hybrid']|length * ceph.disks|length }}"
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds_hybrid'][-1]

# not sure is there a better way to get ruleset id
- name: set ruleset for pool
  shell: ceph osd pool set {{ hybrid_pool }} crush_ruleset
         $(ceph osd crush rule dump |grep {{ hybrid_ruleset }} -A1 |grep -oP '\d+')
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds_hybrid'][-1]
