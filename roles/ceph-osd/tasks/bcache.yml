---
# Create hybrid bucket
- name: Create root hybrid-bucket
  shell: ceph osd crush add-bucket {{ hybrid_pool }} root
  register: result
  changed_when: result.stdout |search("added bucket")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create crush ruleset for the hybrid root bucket
- name: Create crush ruleset hybrid root bucket
  shell: ceph osd crush rule create-simple {{ hybrid_ruleset }} {{ hybrid_pool }} host firstn
  register: result
  changed_when: not result.stdout |search("already exists")
  when: inventory_hostname == groups['ceph_osds_hybrid'][0]

# Create osd host bucket
- name: Create osd host bucket
  shell: ceph osd crush add-bucket {{ ansible_hostname }} host
  register: result
  changed_when: result.stdout |search("added bucket")

# Move the host bucket under corresponding root
- name: Move host bucket under root bucket
  shell: ceph osd crush move {{ ansible_hostname }} root={{ hybrid_pool }}
  register: result
  changed_when: result.stdout |search("moved item")

- name: load dm-cache module
  modprobe: name={{ item }}
  with_items:
    - dm_mod
    - dm_cache

- name: load dm-cache module
  copy: dest=/etc/modules-load.d/{{ item }} content={{ item }}
  with_items:
    - dm_mod
    - dm_cache

# we check for journal and bcache partitions to skip destructive tasks below
- name: check if journal partitions exist on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'journal'"
  ignore_errors: true
  changed_when: false
  register: journal_partitions

- name: check if bcache partition exists on ssd device
  shell: "parted --script /dev/{{ ceph.bcache_ssd_device }} print | egrep -sq 'bcache'"
  ignore_errors: true
  changed_when: false
  register: bcache_partition

- name: initialize ok_to_deploy flag
  set_fact:
    ok_to_deploy: false

- name: set ok_to_deploy flag
  set_fact:
    ok_to_deploy: true
  when: journal_partitions.rc != 0 and
        bcache_partition.rc != 0

- name: mklabel gpt
  command: "parted -s /dev/{{ ceph.bcache_ssd_device }} mklabel gpt"
  when: ok_to_deploy

- name: make journal partitions
  command: parted --script /dev/{{ ceph.bcache_ssd_device }}
           mkpart journal {{ (item|int * 10000) + 1 }}MiB {{ (item|int * 10000) + 10000 }}MiB
  with_sequence: "start=0 end={{ ceph.disks|length - 1 }}"
  when: ok_to_deploy

- name: make bcache block partition
  command: parted --script /dev/{{ ceph.bcache_ssd_device }}
           mkpart bcache {{ ceph.disks|length * 10000 + 1 }}MiB 100%
  when: ok_to_deploy

- name: make xfs on bcache devices
  command: "mkfs.xfs -f -i size=2048 /dev/{{ item }}"
  with_items: "{{ ceph.disks }}"
  when: ok_to_deploy

- name: make metadata device
  command: dmsetup create ssd-metadata --table '0 1024000 linear /dev/sdh7 0'

- name: zero up metadata device
  command: dd if=/dev/zero of=/dev/mapper/ssd-metadata bs=1024000 count=1

- name: cache block device
  command: dmsetup create ssd-blocks --table '0 2219266048 linear /dev/sdh7 1024000'

- name: make-bcache -B <sata disks>
  command: dmsetup create dm{{ item }} --table "0 7812939776 cache /dev/mapper/ssd-metadata /dev/mapper/ssd-blocks /dev/{{ item }} 512 1 writeback default 0"
  with_items: "{{ ceph.disks }}"
  when: ok_to_deploy

- name: make sure all uuids of bcaches exist
  shell: test `ls -l /dev/disk/by-uuid/ |grep dm |wc -l` -eq "{{ ceph.disks|length }}"
  register: result
  until: result.rc == 0
  retries: 6
  delay: 10
  when: ok_to_deploy

- name: activate osds
  ceph_dmcache:
    disks: "{{ ceph.disks }}"
    ssd_device: "{{ ceph.bcache_ssd_device }}"
    journal_guid: "{{ ceph.journal_guid }}"
  when: ok_to_deploy

# pool pg number is based on osd amount
# we should create pool when all osds are up
- name: create openstack pool
  ceph_pool:
    pool_name: "{{ hybrid_pool }}"
    osds: "{{ groups['ceph_osds_hybrid']|length * ceph.disks|length }}"
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when:
    - ok_to_deploy
    - inventory_hostname == groups['ceph_osds_hybrid'][-1]

# not sure is there a better way to get ruleset id
- name: set ruleset for pool
  shell: ceph osd pool set {{ hybrid_pool }} crush_ruleset
         $(ceph osd crush rule dump |grep {{ hybrid_ruleset }} -A1 |grep -oP '\d+')
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when:
    - ok_to_deploy
    - inventory_hostname == groups['ceph_osds_hybrid'][-1]
