---
# wait for up to 6 hours for rebalancing
# this is to tolerate misoperation. This doesn't happen in standard flow.
- name: wait for ceph cluster to be in health ok state for up to 6 hours
  shell: ceph health |grep -P '^(HEALTH_OK)|(HEALTH_WARN \d+ near full osd\(s\))$'
  register: result
  until: result.rc == 0
  retries: 360
  delay: 60
  when:
    - inventory_hostname == groups['ceph_monitors'][0]
    - ceph_scale_out | default(false)

# we restart ceph services when upgrade or config update
# restarting ceph service demands ceph health ok
# Wait for a max of 10 minutes
- name: wait for ceph cluster to be in health ok state
  shell: ceph health |grep -P '^(HEALTH_OK)|(HEALTH_WARN \d+ near full osd\(s\))$'
  register: result
  until: result.rc == 0
  retries: 10
  delay: 60
  delegate_to: "{{ groups['ceph_monitors'][0] }}"

# tasks of ceph monitors
- block:
  - name: stop ceph mon
    service: name={{ ceph.mon_service[os] }} state=stopped

  - name: start ceph mons
    service: name={{ ceph.mon_service[os] }} state=started enabled=yes
  when: "'ceph_monitors' in group_names"

# tasks of ceph osds
- block:
  # Setting noout to prevent cluster from rebalancing after service restart
  - name: set osd noout
    command: ceph osd set noout
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  # Setting osd values to reduce performance degradation of client I/O
  - name: set osd noscrub
    command: ceph osd set noscrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: set osd nodeep scrub
    command: ceph osd set nodeep-scrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: stop ceph osd
    service: name={{ ceph.osd_service[os] }} state=stopped

  - name: start ceph osds
    service: name={{ ceph.osd_service[os] }} state=started enabled=yes

  # make sure ceph osd services are up before unset noout
  - name: make sure ceph osds are up
    shell: test `ls /var/run/ceph/*.asok |wc -l` -eq {{ ceph.disks|length }}
    register: result
    until: result.rc == 0
    retries: 5
    delay: 5

  # Unset the osd values to make heath check pass
  - name: unset osd noout
    command: ceph osd unset noout
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: unset osd noscrub
    command: ceph osd unset noscrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: unset osd nodeep scrub
    command: ceph osd unset nodeep-scrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: "'ceph_osds' in group_names"

- name: remove ceph restart flag
  file: dest=/etc/ansible/facts.d/restart_ceph.fact state=absent

# make sure ceph health ok after update
- name: wait for ceph cluster to be in health ok state
  shell: ceph health |grep -P '^(HEALTH_OK)|(HEALTH_WARN \d+ near full osd\(s\))$'
  register: result
  until: result.rc == 0
  retries: 10
  delay: 60
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds'][-1]
